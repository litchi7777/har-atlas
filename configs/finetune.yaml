# Fine-tuning Configuration

# Dataset
dataset_type: "sensor"

sensor_data:
  dataset_name: "DSADS"
  data_root: "/mnt/home/processed_data/All_30Hz_W150/npy_data2/DSADS"
  mode: "multi_device"
  train_users: [1, 2, 3, 4, 5, 6]
  val_users: [7]
  test_users: [8]

# Model
model:
  name: "simple_cnn"
  backbone: "simple_cnn"
  num_classes: 19
  pretrained_path: null
  freeze_backbone: false

# Data Augmentation
augmentation:
  mode: "light"

# Training
training:
  epochs: 50
  learning_rate: 0.0001  # Usually lower than pre-training
  weight_decay: 0.0001
  optimizer: "adam"
  scheduler: "step"  # step, cosine, plateau
  step_size: 20
  gamma: 0.1

# Loss Function
loss:
  type: "cross_entropy"  # cross_entropy, focal_loss, etc.
  label_smoothing: 0.1

# Hardware
device: "cuda"
multi_gpu: false
mixed_precision: true

# Checkpointing
checkpoint:
  save_best: true
  metric: "val_accuracy"  # val_accuracy, val_loss
  save_path: "models/finetuned"
  resume: null

# Evaluation
evaluation:
  eval_interval: 1  # evaluate every N epochs
  metrics: ["accuracy", "f1", "precision", "recall"]

# Logging
logging:
  log_interval: 10
  log_dir: "logs/finetune"

# Weights & Biases
wandb:
  enabled: true
  project: "har-foundation"
  entity: null  # Your W&B username or team name
  name: null  # Run name (auto-generated if null)
  tags: ["finetune", "classification"]
  notes: "Supervised fine-tuning"

# Early Stopping
early_stopping:
  patience: 10
  min_delta: 0.001

# Seed for reproducibility
seed: 42

# Grid Search
grid_search:
  training:
    learning_rate: [0.0001]

settings:
  stop_on_error: false
  save_summary: true
  summary_path: "logs/finetune_experiments_summary.json"
