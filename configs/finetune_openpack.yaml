# Fine-tuning Configuration

# Dataset
dataset_type: "sensor"

sensor_data:
  # データセットの基本設定
  data_root: "har-unified-dataset/data/processed"  # データルートディレクトリ
  datasets: ["openpack"]  # OPENPACKデータセット

  # バッチローダー設定
  batch_loader:
    exclude_patterns: []  # 除外パターン（例: ["USER00001"]）
    sample_threshold: 50  # 使用しないが、バリデーションのため保持
    batch_size: 64  # バッチサイズ

  # ユーザー分割設定
  user_split:
    test_users: ["USER00001", "USER00002"]  # テスト用ユーザーID
    val_users: ["USER00003", "USER00004"]   # 検証用ユーザーID
    # train_users は指定なし（test/val以外の全ユーザー）

# Model
model:
  name: "resnet"
  backbone: "resnet"
  num_classes: 6  # OPENPACKのクラス数（自動検出される）
  pretrained_path: null  # 事前学習済みモデルのパス（nullの場合はランダム初期化）
  freeze_backbone: false

# Data Augmentation
augmentation:
  mode: "light"

# Training
training:
  epochs: 50
  learning_rate: 0.0001  # Usually lower than pre-training
  weight_decay: 0.0001
  optimizer: "adam"
  scheduler: "step"  # step, cosine, plateau
  step_size: 20
  gamma: 0.1

# Loss Function
loss:
  type: "cross_entropy"  # cross_entropy, focal_loss, etc.
  label_smoothing: 0.1

# Hardware
device: "cuda"
multi_gpu: false
mixed_precision: true

# Checkpointing
checkpoint:
  save_best: true
  metric: "val_accuracy"  # val_accuracy, val_loss
  resume: null

# Evaluation
evaluation:
  eval_interval: 1  # evaluate every N epochs
  metrics: ["accuracy", "f1", "precision", "recall"]

# Logging
logging:
  log_interval: 10
  log_dir: "logs/finetune"

# Weights & Biases
wandb:
  enabled: true
  project: "har-foundation"
  entity: null  # Your W&B username or team name
  name: null  # Run name (auto-generated if null)
  tags: ["finetune", "classification"]
  notes: "Supervised fine-tuning"

# Early Stopping
early_stopping:
  patience: 10
  min_delta: 0.001

# Seed for reproducibility
seed: 42

# Grid Search
grid_search:
  model:
    pretrained_path:
      - null  # ランダム初期化（ベースライン）
      - "experiments/pretrain/run_20251029_171342/checkpoints/best_model.pth"  # epoch 40
      - "experiments/pretrain/run_20251029_171512/checkpoints/best_model.pth"  # epoch 100
      - "experiments/pretrain/run_20251029_060659/checkpoints/best_model.pth"  # epoch 50 (別実験)

settings:
  stop_on_error: false
  save_summary: true
  summary_path: "logs/finetune_openpack_experiments_summary.json"
  parallel: true  # GPU並列実行を有効化
  max_workers: null  # null=利用可能なGPU全て、数値=最大並列数
