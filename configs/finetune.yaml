# Fine-tuning Configuration

# Dataset
dataset_type: "sensor"

sensor_data:
  # データセットの基本設定
  data_root: "har-unified-dataset/data/processed"  # データルートディレクトリ

  # データセットとセンサー位置のペア（grid_searchで上書き）
  # 各ペアは [データセット名, センサー位置] の形式
  dataset_location_pairs:
    - ["dsads", "Torso"]  # デフォルト: DSADS Torso

  # バッチローダー設定
  batch_loader:
    exclude_patterns: []  # 除外パターン（例: ["USER00001"]）
    sample_threshold: 50  # 使用しないが、バリデーションのため保持
    batch_size: 200  # バッチサイズ（前の設定に合わせて高速化）

  # ユーザー分割設定
  user_split:
    test_users: ["1", "2"]  # テスト用ユーザーID
    val_users: ["3", "4"]   # 検証用ユーザーID
    # train_users は指定なし（test/val以外の全ユーザー）

# Model
model:
  name: "resnet"
  backbone: "resnet"
  num_classes: 6  # NHANESのクラス数（後で自動検出される）
  pretrained_path: null  # 事前学習済みモデルのパス（nullの場合はランダム初期化）
  freeze_backbone: false

# Data Augmentation
augmentation:
  mode: "light"

# Training
training:
  epochs: 20  # 前の設定に合わせて20エポックに変更
  learning_rate: 0.0001  # 前の設定に合わせて変更
  weight_decay: 0.0001
  optimizer: "adam"
  scheduler: "step"  # step, cosine, plateau
  step_size: 20
  gamma: 0.1
  max_samples_per_epoch: 5000  # 1エポックあたり5000サンプルに制限（学習時間短縮）

# Loss Function
loss:
  type: "cross_entropy"  # cross_entropy, focal_loss, etc.
  label_smoothing: 0.1

# Hardware
device: "cuda"
multi_gpu: false
mixed_precision: true

# Checkpointing
checkpoint:
  save_best: true
  metric: "val_accuracy"  # val_accuracy, val_loss
  resume: null

# Evaluation
evaluation:
  eval_interval: 1  # evaluate every N epochs
  metrics: ["accuracy", "f1", "precision", "recall"]

# Weights & Biases
wandb:
  enabled: true
  project: "har-foundation"
  group: null  # 自動生成（job_type + タイムスタンプ）。手動設定も可能
  job_type: "finetune"
  entity: null
  name: null
  tags: []
  notes: null

# Early Stopping
early_stopping:
  patience: 5  # 早めに収束判定（学習時間短縮）
  min_delta: 0.0001

# Seed for reproducibility
seed: 42

# Grid Search
# 1エポックあたりのサンプル数の比較実験（エポック数：20で固定）
grid_search:
  sensor_data:
    dataset_location_pairs:
      # DSADS Torsoで固定（シングルデバイス）
      - [["dsads", "Torso"]]
  model:
    pretrained_path:
      - null  # ランダム初期化のみ
  training:
    epochs:
      - 20  # エポック数を20で固定
    max_samples_per_epoch:
      - 40000  # 超大量（8倍のデータ）
      - 80000  # 極大量（16倍のデータ）
settings:
  stop_on_error: false
  save_summary: true
  # summary_pathを指定しない場合、自動的にexperiments/*/run_*/summary.jsonに保存される
  parallel: true  # GPU並列実行を有効化
  max_workers: 2  # 2実験を並列実行（max_samples_per_epoch: 40k, 80k）
  available_gpus: [0, 1]  # 使用するGPU（2つ）
