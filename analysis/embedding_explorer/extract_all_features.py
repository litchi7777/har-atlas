"""
å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã®ç‰¹å¾´æŠ½å‡ºã‚’ä¸€æ‹¬å®Ÿè¡Œ

ä½¿ç”¨æ–¹æ³•:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã§å®Ÿè¡Œ
    python analysis/embedding_explorer/extract_all_features.py

    # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã§å®Ÿè¡Œ
    python analysis/embedding_explorer/extract_all_features.py \
        --model-5-0s experiments/pretrain/run_*/exp_2/models/checkpoint.pth \
        --model-2-0s experiments/pretrain/run_*/exp_0/models/checkpoint.pth \
        --model-1-0s experiments/pretrain/run_*/exp_1/models/checkpoint.pth \
        --model-0-5s experiments/pretrain/run_*/exp_2/models/checkpoint.pth

    # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•æ¤œå‡º
    python analysis/embedding_explorer/extract_all_features.py --auto-detect
"""

import os
import sys
import argparse
import subprocess
import glob
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
project_root = Path(__file__).parent.parent.parent


def find_latest_checkpoint(pattern, window_size_desc=""):
    """
    æŒ‡å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã§æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œç´¢

    Args:
        pattern: ã‚°ãƒ­ãƒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: "experiments/pretrain/*/exp_0/models/checkpoint_epoch_*.pth"ï¼‰
        window_size_desc: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã®èª¬æ˜Žï¼ˆãƒ­ã‚°ç”¨ï¼‰

    Returns:
        æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹ï¼ˆstrï¼‰ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
    """
    checkpoints = glob.glob(str(project_root / pattern))
    if not checkpoints:
        print(f"  âš ï¸  No checkpoints found for pattern: {pattern}")
        return None

    # epochç•ªå·ã§ã‚½ãƒ¼ãƒˆï¼ˆæ•°å€¤ã¨ã—ã¦ï¼‰
    def extract_epoch(path):
        try:
            basename = Path(path).name
            if "epoch_" in basename:
                epoch_str = basename.split("epoch_")[1].split(".pth")[0]
                return int(epoch_str)
        except:
            return 0
        return 0

    latest = max(checkpoints, key=extract_epoch)
    epoch = extract_epoch(latest)
    print(f"  âœ“ Found {window_size_desc}: {latest} (epoch {epoch})")
    return latest


def extract_features(model_path, max_samples=100, max_users=20, output_dir=None, device='cuda', output_file=None):
    """
    ç‰¹å¾´æŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ

    Args:
        model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        max_users: æœ€å¤§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        device: ãƒ‡ãƒã‚¤ã‚¹ ('cuda' or 'cpu')
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: "features_2.0s.npz")

    Returns:
        å®Ÿè¡ŒæˆåŠŸæ™‚ã¯Trueã€å¤±æ•—æ™‚ã¯False
    """
    if not Path(model_path).exists():
        print(f"  âŒ Model file not found: {model_path}")
        return False

    if output_dir is None:
        output_dir = project_root / "analysis" / "embedding_explorer" / "data"

    cmd = [
        sys.executable,
        str(project_root / "analysis" / "embedding_explorer" / "extract_features.py"),
        "--model", str(model_path),
        "--max-samples", str(max_samples),
        "--max-users", str(max_users),
        "--output-dir", str(output_dir),
        "--device", device
    ]

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®š
    if output_file is not None:
        cmd.extend(["--output-file", str(output_file)])

    print(f"  Running: {' '.join(cmd[1:])}")
    print()

    result = subprocess.run(cmd, cwd=str(project_root))
    return result.returncode == 0


def main():
    parser = argparse.ArgumentParser(
        description='Extract features for all window sizes',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹æŒ‡å®š
    parser.add_argument('--model-5-0s', type=str, default=None, dest='model_5_0s',
                        help='Model path for 5.0s (150 samples)')
    parser.add_argument('--model-2-0s', type=str, default=None, dest='model_2_0s',
                        help='Model path for 2.0s (60 samples)')
    parser.add_argument('--model-1-0s', type=str, default=None, dest='model_1_0s',
                        help='Model path for 1.0s (30 samples)')
    parser.add_argument('--model-0-5s', type=str, default=None, dest='model_0_5s',
                        help='Model path for 0.5s (15 samples)')

    # è‡ªå‹•æ¤œå‡º
    parser.add_argument('--auto-detect', action='store_true',
                        help='Auto-detect latest model checkpoints')

    # å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--max-samples', type=int, default=100,
                        help='Max samples per class per dataset-location')
    parser.add_argument('--max-users', type=int, default=20,
                        help='Max users for large datasets')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='Output directory')
    parser.add_argument('--device', type=str, default='cuda',
                        help='Device (cuda or cpu)')
    parser.add_argument('--skip-existing', action='store_true',
                        help='Skip if output file already exists')

    args = parser.parse_args()

    print("=" * 60)
    print("Feature Extraction for All Window Sizes")
    print("=" * 60)
    print()

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    if args.output_dir is None:
        output_dir = project_root / "analysis" / "embedding_explorer" / "data"
    else:
        output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®è¨­å®š
    models = {}

    if args.auto_detect or all([args.model_5_0s is None, args.model_2_0s is None,
                                  args.model_1_0s is None, args.model_0_5s is None]):
        print("ðŸ” Auto-detecting model checkpoints...")
        # 5.0s: window_size=150 (é€šå¸¸ã¯exp_2)
        models['5.0s'] = find_latest_checkpoint(
            "experiments/pretrain/*/exp_2/models/checkpoint_epoch_*.pth",
            "5.0s (150 samples)"
        )
        # Fallback
        if models['5.0s'] is None:
            models['5.0s'] = "experiments/pretrain/run_20251111_171703/exp_2/models/checkpoint_epoch_45.pth"
            print(f"  Using fallback: {models['5.0s']}")

        # 2.0s: window_size=60 (é€šå¸¸ã¯exp_0)
        models['2.0s'] = find_latest_checkpoint(
            "experiments/pretrain/run_20251112_*/exp_0/models/checkpoint_epoch_*.pth",
            "2.0s (60 samples)"
        )
        if models['2.0s'] is None:
            models['2.0s'] = "experiments/pretrain/run_20251112_192545/exp_0/models/checkpoint_epoch_40.pth"
            print(f"  Using fallback: {models['2.0s']}")

        # 1.0s: window_size=30 (é€šå¸¸ã¯exp_1)
        models['1.0s'] = find_latest_checkpoint(
            "experiments/pretrain/run_20251112_*/exp_1/models/checkpoint_epoch_*.pth",
            "1.0s (30 samples)"
        )
        if models['1.0s'] is None:
            models['1.0s'] = "experiments/pretrain/run_20251112_192545/exp_1/models/checkpoint_epoch_40.pth"
            print(f"  Using fallback: {models['1.0s']}")

        # 0.5s: window_size=15 (é€šå¸¸ã¯exp_2 in run_20251112*)
        models['0.5s'] = find_latest_checkpoint(
            "experiments/pretrain/run_20251112_*/exp_2/models/checkpoint_epoch_*.pth",
            "0.5s (15 samples)"
        )
        if models['0.5s'] is None:
            models['0.5s'] = "experiments/pretrain/run_20251112_192545/exp_2/models/checkpoint_epoch_39.pth"
            print(f"  Using fallback: {models['0.5s']}")
        print()
    else:
        # æ‰‹å‹•æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨
        models['5.0s'] = args.model_5_0s or "experiments/pretrain/run_20251111_171703/exp_2/models/checkpoint_epoch_45.pth"
        models['2.0s'] = args.model_2_0s or "experiments/pretrain/run_20251112_192545/exp_0/models/checkpoint_epoch_40.pth"
        models['1.0s'] = args.model_1_0s or "experiments/pretrain/run_20251112_192545/exp_1/models/checkpoint_epoch_40.pth"
        models['0.5s'] = args.model_0_5s or "experiments/pretrain/run_20251112_192545/exp_2/models/checkpoint_epoch_39.pth"

    print("=" * 60)
    print("Starting feature extraction...")
    print("=" * 60)
    print()

    success_count = 0
    total_count = len(models)

    for i, (window_size, model_path) in enumerate(models.items(), 1):
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        output_file = output_dir / f"features_{window_size}.npz"
        if args.skip_existing and output_file.exists():
            print(f"ðŸ“Š [{i}/{total_count}] Skipping {window_size} (file exists)")
            print(f"  Output: {output_file}")
            print()
            success_count += 1
            continue

        print(f"ðŸ“Š [{i}/{total_count}] Extracting features for {window_size}...")
        print(f"  Model: {model_path}")

        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ˜Žç¤ºçš„ã«æŒ‡å®š
        output_filename = f"features_{window_size}"
        success = extract_features(
            model_path,
            max_samples=args.max_samples,
            max_users=args.max_users,
            output_dir=output_dir,
            device=args.device,
            output_file=output_filename
        )

        if success:
            print(f"  âœ“ {window_size} features extracted successfully")
            success_count += 1
        else:
            print(f"  âŒ Failed to extract {window_size} features")

        print()

    print("=" * 60)
    if success_count == total_count:
        print(f"âœ“ All {total_count} feature sets extracted successfully!")
    else:
        print(f"âš ï¸  {success_count}/{total_count} feature sets extracted successfully")
    print("=" * 60)
    print()
    print("Output files:")

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    for file in sorted(output_dir.glob("*")):
        size_mb = file.stat().st_size / 1024 / 1024
        print(f"  {file.name:30s} {size_mb:>8.2f} MB")

    print()
    print("Next step: Start the server with:")
    print("  python analysis/embedding_explorer/server.py --port 8050 --debug")


if __name__ == '__main__':
    main()
